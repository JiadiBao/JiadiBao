---
title: "Stochastic Variational Inference for Gaussian Mixture Model"
layout: post
date: 2023-02-17 21:39
headerImage: false
tag:
- optimization
- generative model
star: true
category: blog
author: JiadiBao
description: SVI-GMM
---

#### *Last updated at 02-17-2023*

Variational Inference is an irreplaceable approach for the hidden variable inference for a probabilistic model. Deriving the closed-form update function required a lot of work. The auto-differential provided by Tensorflow reduces the difficulties of equation deriving. The stochastic variational inference for Gaussian Mixture Model is introduced.

There are some useful materials, which I believe are helpful for understanding the beautiful method of Variation Inference.

- D. M. Blei, A. Kucukelbir, and J. D. McAuliffe, “Variational Inference: A Review for Statisticians,” J. Am. Stat. Assoc., vol. 112, no. 518, pp. 859–877, 2017.
- https://zhiyzuo.github.io/VI/
- https://github.com/brendanhasz/svi-gaussian-mixture-model
- Hoffman M D, Blei D M, Wang C, et al. Stochastic variational inference[J]. Journal of Machine Learning Research, 2013.

Note that this blog is the conclusion of the learning of below materials. It is encouraged to read below materials before my blog.

---

```python
import numpy as np
import tensorflow as tf
import tensorflow_probability as tfp
tfd = tfp.distributions
```

```python
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline
```

Tensorflow provided a useful tool for probability programming: ***tensorflow_probability***, with the help of tensorflow optimizer, It is easy for us to solve the variational optimization problem.

First of all, we introduce a motivating example of the Gaussian Mixture model.

## Gaussian Mixture Model (GMM)

This part is basically the same as the blog written by prof. Zuo in https://zhiyzuo.github.io/VI/.

To simplify the problem, the example data is set to be univariate. There will be ***K*** components, each component follows the univariate Gaussian Distribution with means $\boldsymbol{\mu}=\{\mu_1, \mu_2,...\mu_K\}$, and variance $\boldsymbol{\sigma}^2=\{\sigma_1^2,\sigma_2^2,...\sigma_K^2\}$. To simplify the problem, the variance is set as one.

In a Bayesian setting, the random variable exists because of its prior distribution. In GMM, the prior distribution of the mean and the variance is assumed to follow a ***Gaussian-Gamma distribution***：
$$
\mu_k|\sigma_k^2 \sim Normal(l_0,s_0)
$$

$$
\sigma_k^2 \sim Gamma(\alpha_0,\beta_0)
$$

The cluster labels (The probability of a data point being generated by any of the mixture components) follow a Categorical distribution:
$$
c^{(i)}\sim Categorical(\theta)
$$
To ensure the graphical model is conjugate, which will enable the closed-form update function of variational inference, the prior distribution of $\theta$ follows the Dirichlet distribution:
$$
\theta_k \sim Dirichlet([\frac{1}{K},...,\frac{1}{K}])
$$


Where $l,s$ are short for loc and scale of the Gaussian distribution. We can set a very easy ***generative model***. 

- For each data point $x^i$
  - Sample a cluster assignment $c^i$ from the uniform distribution: $c^i \sim Uniform(K)$.
  - Sample its value from the corresponding component: $x^i\sim Normal(\mu_{c_i})$

To simplify this setting, we shall generate the Gaussian distribution and add mean shifts:

```python
N = 3000
X = np.random.randn(N, 2).astype('float32')
X[:1000, :] += [5, 0]
X[1000:2000, :] -= [2, 4]
X[2000:, :] += [-2, 4]
```

Plot the data:

```python
plt.plot(X[:, 0], X[:, 1], '.')
plt.axis('equal')
plt.show()
```

![Image](https://JiadiBao.github.io/assets/images/SVIGMM/Data.pdf)

There are three variables need to be considered in the Gaussian Mixuture Model: the cluster label $c$, the Gaussian mean $\boldsymbol{\mu}$, and the observed data $\boldsymbol{x}$. The joint likelihood function is easy to factorized via Bayesian Model:
$$
\begin{aligned}
p(\boldsymbol{c,\mu,x})
&=p(\boldsymbol{\mu})p(\boldsymbol{c})p(\boldsymbol{\mu,c|x})\\
&=\prod_{i=1}^K{p(\mu_i)p(c^{(i)})}p(\mu_i,c^{(i)}|x^{(i)})
\end{aligned}
$$
The likelihood of the data $x$ can be easily obtained through intergrating the latent random variable.
$$
\begin{aligned}
p(\boldsymbol{x})&=\int p(\boldsymbol{c,\mu,x})d\boldsymbol{c}d\boldsymbol{\mu}\\
&=\int \prod_{i=1}^K{p(\mu_i)p(c^{(i)})}p(\mu_i,c^{(i)}|x^{(i)})d\boldsymbol{c}d\boldsymbol{\mu}
\end{aligned}
$$
However the intergal is often intractable, MCMC or VI are two viable solutions. Compare these two algorithms, Viriational inference is highly efficient than MCMC, especially for the large scale of datasets. However, there are some shortcomings of VI : first, the viriational inference is optimized via coordinate ascent algorithm, the optimal parameters are easily falling into the local optima, and MCMC has been proven that it can sufficiently covergence to the global optima despite the time consuming.

## A glimpse of Variantional Inference

Suppose we have a series of hidden random variable $\boldsymbol{z}=z^{(1)},z^{(2)},...,z^{(N)}$ and a series of observed data $\boldsymbol{x}=x^{(1)},x^{(2)},...,x^{(N)}$. The inference problem is to apply the observed data to calculate the posteiror distribution $p(\boldsymbol{z|x})$.
$$
p(\boldsymbol{z|x})=\frac{p(\boldsymbol{z,x})}{p(\boldsymbol{x})}=\frac{p(\boldsymbol{x|z})p(\boldsymbol{z})}{\int p(\boldsymbol{x,z}) d\boldsymbol{z}}
$$
The denominator is the well-known evidence lower bound. The exact inference of the evidence is always intractable. The viriational inference is an alternative solution.

The likelihood of the observed data can be decomposed as follows:
$$
\begin{aligned}
\ln p(x)&=\int_z q(\boldsymbol{z}) d\boldsymbol{z} ~\ln p(x)\\

&=\int_z q(\boldsymbol{z})\ln \frac{p(\boldsymbol{x,z})}{p(\boldsymbol{z|x})}d\boldsymbol{z}\\
&=\int_z q(\boldsymbol{z})\ln \frac{p(\boldsymbol{x,z})q(\boldsymbol{z})}{p(\boldsymbol{z|x})q(\boldsymbol{z})}d\boldsymbol{z}\\
&=\int_z q(\boldsymbol{z})\ln \frac{p(\boldsymbol{x,z})}{q(\boldsymbol{z})}d\boldsymbol{z}+\int_z q(\boldsymbol{z})\ln \frac{p(\boldsymbol{z|x})}{q(\boldsymbol{z})}d\boldsymbol{z}\\
&=\mathcal{L}(\boldsymbol{x})+\mathcal{KL}(q||p)
\end{aligned}
$$
The essence of the Variational inference is to come up with a suggorate distribution and to minimize the distance of the suggorate distribution and the true posterior distribution. $\mathcal{KL}$ divergence is a popular way to measure the distance of two distributions. It is proven by Jensen‘s inequality that minimizes the $\mathcal{KL}$ divergence is equal to maximum the evidence lower bound($\mathcal{L}$)。

The next step is to decompose the evidence lower bound:
$$
\begin{aligned}
\mathcal{L}(\boldsymbol{x})&=\int_zq(\boldsymbol{z})\ln\frac{p(\boldsymbol{x,z})}{q(\boldsymbol{z})}\\
&=\int_zq(\boldsymbol{z})\ln p(\boldsymbol{x|z})-q(\boldsymbol{z})\ln\frac{q(\boldsymbol{z})}{p(\boldsymbol{z})}\\
&=E_q[\ln p(\boldsymbol{x|z})]-\mathcal{KL}(q\boldsymbol(z)||p(\boldsymbol{z}))\\
\end{aligned}
$$
It can be seen that the evidence lower bound trades off between the two terms:

- The first term prefers $q(\boldsymbol{z})$ to be high when the complete likelihood $p(\boldsymbol{x,z})$ is high.
- The second term encourages $q(\boldsymbol{z})$ to be diffused across the space.

Then we should make a Tensorfolw data set for the simulation data. Data needs to be shuffled and put in to a data batch.

```python
batch_size = 500
dataset = tf.data.Dataset.from_tensor_slices((X)).shuffle(10000).batch(batch_size)
```

## Construction of a TensorFlow model

We shall use the TensorFlow auto-differentiate tool to optimize the evidence lower bound.

We will use the model given by Tensorflow

```python
class GaussianMixtureModel(tf.keras.Model):
   def __init__(self,):
   def call(self,):
```

In the TensorFlow constructor, we shall create all variables and define the prior distributions. 

We have illustrated the case of univariate Gaussian distribution. The two-dimensional Gaussian case is equal to parallel processing the two univariate model.

```python
def __init__(self, Nc, Nd):
    # Initialize
    self.Nc = Nc # the number of clusters
    self.Nd = Nd # the number of dimensions

    # Variational distribution variables for means
    self.locs = tf.Variable(tf.random.normal((Nc, Nd)))
    self.scales = tf.Variable(tf.pow(tf.random.gamma((Nc, Nd), 5, 5), -0.5))

    # Variational distribution variables for standard deviations
    self.alpha = tf.Variable(tf.random.uniform((Nc, Nd), 4., 6.))
    self.beta = tf.Variable(tf.random.uniform((Nc, Nd), 4., 6.))

    # Variational distribution variables for component weights
    self.counts = tf.Variable(2*tf.ones((Nc,)))

    # Prior distributions for the means
    self.mu_prior = tfd.Normal(tf.zeros((Nc, Nd)), tf.ones((Nc, Nd)))

    # Prior distributions for the standard deviations
    self.sigma_prior = tfd.Gamma(5*tf.ones((Nc, Nd)), 5*tf.ones((Nc, Nd)))

    # Prior distributions for the component weights
    self.theta_prior = tfd.Dirichlet(2*tf.ones((Nc,)))
```



In the tensorflow call method, we shall calculate two objectives. First, the likelihood of each datapoint in the batch, given the model and all hidden variables. Second, the KL divergence between the variational posteriors.

```python
def call(self, x, sampling=True, independent=True):
  # The variational distributions
  mu = tfd.Normal(self.locs, self.scales)
  sigma = tfd.Gamma(self.alpha, self.beta)
  theta = tfd.Dirichlet(self.counts)

  # Sample from the variational distributions
  if sampling:
    Nb = x.shape[0] #number of samples in the batch
    mu_sample = mu.sample(Nb)
    sigma_sample = tf.pow(sigma.sample(Nb), -0.5)
    theta_sample = theta.sample(Nb)
  else:
    mu_sample = tf.reshape(mu.mean(), (1, self.Nc, self.Nd))
    sigma_sample = tf.pow(tf.reshape(sigma.mean(), (1, self.Nc, self.Nd)), -0.5)
    theta_sample = tf.reshape(theta.mean(), (1, self.Nc))

    # The mixture density
    density = tfd.Mixture(cat=tfd.Categorical(probs=theta_sample),
      										components=[tfd.MultivariateNormalDiag(loc=mu_sample[:, i, :], 	
                          scale_diag=sigma_sample[:, i, :])for i in range(self.Nc)])

    # Compute the mean log likelihood
    log_likelihoods = density.log_prob(x)

    # Compute the KL divergence sum
    mu_div    = tf.reduce_sum(tfd.kl_divergence(mu,    self.mu_prior))
    sigma_div = tf.reduce_sum(tfd.kl_divergence(sigma, self.sigma_prior))
    theta_div = tf.reduce_sum(tfd.kl_divergence(theta, self.theta_prior))
    kl_sum = mu_div + sigma_div + theta_div

    # Return both losses
    return log_likelihoods, kl_sum
```

## Optimization

After the declare the model, The TensorFlow optimizer is available for variational parameter optimization.

```python
model = GaussianMixtureModel(3, 2)
optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)
```

 After declaring the optimizer, the model training step, aka auto-optimization step is illustrated as follows:

```python
def train_step(data):
    with tf.GradientTape() as tape:
        log_likelihoods, kl_sum = model(data)
        elbo_loss = kl_sum/N - tf.reduce_mean(log_likelihoods)
    gradients = tape.gradient(elbo_loss, model.trainable_variables)
    optimizer.apply_gradients(zip(gradients, model.trainable_variables))
```

As we derived before, the loss of the Gaussian Mixture Model is **negative evidence lower bound**. Two factors are illustrated in equation (9). Model fitting step is illustrated as follows:

```python
EPOCHS = 1000
for epoch in range(EPOCHS):
    for data in dataset:
        train_step(data)
```



